{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\provo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\provo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\provo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('data_train.pkl', allow_pickle=True)\n",
    "test_data = np.load('data_test.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')    # https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer\n",
    "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    def train(self, data):\n",
    "        # Distinguish comments and labels\n",
    "        comments = data[0]\n",
    "        labels = np.array(data[1])\n",
    "        unique_labels = np.unique(labels)\n",
    "        \n",
    "        # Initialize list of bags\n",
    "        self.bags = []\n",
    "\n",
    "        # Iterate over all classes\n",
    "        j = 0\n",
    "        for label in unique_labels:\n",
    "            s = ''\n",
    "            # Find all entries for that class\n",
    "            indices = np.where(labels == label)[0]\n",
    "            # Iterate over those entries\n",
    "            for ind in indices:\n",
    "                # Add comment to string\n",
    "                s += str(comments[ind])\n",
    "            \n",
    "            # Split string into words\n",
    "            s = self.tokenizer.tokenize(s)\n",
    "            \n",
    "            # Remove stop words\n",
    "            s = [w for w in s if w not in stopwords.words('english')]\n",
    "            \n",
    "            # Convert words to their base form\n",
    "            s = [self.wnl.lemmatize(w) for w in s]\n",
    "            \n",
    "            # Count word frequency and calculate log probabilities\n",
    "            words, counts = np.unique(s, return_counts=True)\n",
    "            probs = np.log(counts+1 / np.sum(counts+1))\n",
    "            \n",
    "            # Create bag of words\n",
    "            self.bags.append((words,counts,probs))\n",
    "            \n",
    "            # Print progress\n",
    "            j+=1\n",
    "            print(str(j) + '/20', end='\\r')\n",
    "    \n",
    "    def predict(self, data):\n",
    "        # Initialize list of predictions\n",
    "        predictions = []\n",
    "        j = 1\n",
    "        \n",
    "        # Iterate over comments\n",
    "        for comment in data:\n",
    "            # Split comment using tokenizer\n",
    "            comment = self.tokenizer.tokenize(comment)\n",
    "\n",
    "            # Initialize list of class probabilities\n",
    "            self.probabilities = [0 for i in range(20)]\n",
    "            \n",
    "            # Iterate over bags of words\n",
    "            for prob_idx, bag in enumerate(self.bags):\n",
    "                # Iterate over words in \n",
    "                for word in comment:\n",
    "                    # Find index of word in bag (if possible)\n",
    "                    bag_idx = np.where(bag[0] == word)[0]\n",
    "                    \n",
    "                    # Add word probability to class probability\n",
    "                    if bag_idx.size > 0:\n",
    "                        self.probabilities[prob_idx] += bag[2][bag_idx][0]\n",
    "                    else:\n",
    "                        self.probabilities[prob_idx] += np.log(1/(np.sum(bag[1]+1)))\n",
    "                        \n",
    "            # Predict class with highest probability\n",
    "            predictions.append(np.argmax(self.probabilities))\n",
    "            \n",
    "            # Print progress\n",
    "            print(str(j) + '/' + str(len(data)), end='\\r')\n",
    "            j += 1\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(predictions):\n",
    "    f = open('submission.csv', 'w')\n",
    "    f.close()\n",
    "    f = open('submission.csv', 'a')\n",
    "    f.write('Id,Category\\n')\n",
    "    for idx, pred in enumerate(predictions):\n",
    "        f.write(str(idx) + ',' + classes[pred] + '\\n')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20\r"
     ]
    }
   ],
   "source": [
    "clf.train((train_data[0][:5000], train_data[1][:5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000\r"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(train_data[0][-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.316\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(train_data[1])\n",
    "correct = 0\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if classes[pred] == train_data[1][69000+idx]:\n",
    "        correct += 1\n",
    "print(correct/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.357\n",
    "np.sum([1,2,3,4])\n",
    "test = np.array([1,2,3,4])\n",
    "np.sum(test + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
