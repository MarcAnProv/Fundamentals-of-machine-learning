# Fundamentals-of-machine-learning

This course is an introduction to machine learning (ML), a field of research in artificial intelligence. 

The course will cover the following subjects: general notions (basic terminology, generalization, curse of dimensionality, capacity, classifier comparison), supervised algorithms (k-nearest neighbors, linear classifiers, neural networks, support vector machines, decision trees and regression, ensemble methods), unsupervised algorithms (principal component analysis, k-means method), overview of probabilistic graphical models.

Notions covered (indicative titles):

* Introduction and terminology of learning.
* Data Representation (+ Linear Algebra Reminders.)
* The tasks of supervised learning (classification, regression) and unsupervised (density estimation, clustering, dimensionality reduction, …)
* Histogram-based methods. Curse of dimensionality. (+ reminders of probability and statistics)
* Neighborhood-based methods (nearest neighbors and Parzen windows)
* Multivariate Gaussian density
* General framework of learning, performance evaluation, model selection, concept of capacity.
* Principle of maximum likelihood v.s. Bayesian inference. Bayes classifier.
* Regression and linear classification (logistic regression, perceptron, linear SVM, …)
* Classification and non-linear regression with kernel trick (+ optimization techniques)
* Classification and non-linear regression: MLP-type neural networks (+ revision of calculation of partial derivatives)
* More advanced neural network architectures: convolutional, recurrent.
* Decision trees.
* Aggregation methods: bagging and boosting
* Unsupervised learning: dimensionality reduction (PCA, …). Partitioning (k-means, …)
* Overview of probabilistic graphical models
* Introduction to deep learning
* Convolutional neural netoworks
* Recurrent neural networks

For more info, see the [class website].(http://mitliagkas.github.io/ift6390-ml-class-2019/)
